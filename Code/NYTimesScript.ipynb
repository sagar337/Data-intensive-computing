{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name:Soumitra Alate UBITName: smalate(50289133)\n",
    "#Name:Sagar Pokale   UBITName: sagarpok(50288055)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nytimesarticle in c:\\users\\antho\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\antho\\anaconda3\\lib\\site-packages (from nytimesarticle) (2.21.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\antho\\anaconda3\\lib\\site-packages (from requests>=2.1.0->nytimesarticle) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\antho\\anaconda3\\lib\\site-packages (from requests>=2.1.0->nytimesarticle) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\antho\\anaconda3\\lib\\site-packages (from requests>=2.1.0->nytimesarticle) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\antho\\anaconda3\\lib\\site-packages (from requests>=2.1.0->nytimesarticle) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install nytimesarticle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\antho\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\antho\\anaconda3\\lib\\site-packages (from bs4) (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nytimesarticle import articleAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "\n",
    "api = articleAPI(\"ymTkPLNZGW2odtHdOPGtOadEvqoTUldn\")\n",
    "\n",
    "links=[]\n",
    "n=0\n",
    "try:\n",
    "    for a in range(0,1000):\n",
    "        articles = api.search(q=\"sport\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "     print(\"error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"baseball\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"soccer\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"cricket\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"ice hockey\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"football\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"athletics\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"american football\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"motorsport\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"Tennis\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"NBA\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"ashes\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            #links.append(url)\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"FIFA\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"IPL\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a in range(0,100):\n",
    "        articles = api.search(q=\"World Cup\", begin_date=20190101,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            filename='top'+str(n)+'.txt'\n",
    "            f=open(filename,'w')\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            f.close()\n",
    "            n+=1\n",
    "    \n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference for merging text files :https://stackoverflow.com/questions/13613336/python-concatenate-text-files\n",
    "import glob\n",
    "\n",
    "read_files = glob.glob(\"*.txt\")\n",
    "\n",
    "with open(\"result.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\antho\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\antho\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\antho\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import io \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =set(stopwords.words('english')) \n",
    "with open('result.txt', 'r') as f:\n",
    "        file= open('NYTimes.txt','a')\n",
    "        for line in f:\n",
    "            singles = []\n",
    "            \n",
    "            stemmer = PorterStemmer() \n",
    "            for plural in line.split():\n",
    "                singles.append(stemmer.stem(plural))\n",
    "                #file.write(' '.join(singles))\n",
    "            #print(' '.join(singles))\n",
    "            res = ' '.join(singles)\n",
    "            cleanText = re.sub(r'[^\\w\\s]','',res)\n",
    "            resulttext = re.sub(r'[0-9]', '', cleanText)\n",
    "            print(\"cleantext\")\n",
    "            print(cleanText)\n",
    "            words = resulttext.split()\n",
    "            for r in words: \n",
    "                if not r in stop_words:\n",
    "                    print(r)\n",
    "                    file.write(\" \"+r) \n",
    "            \n",
    "        file.close()\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
